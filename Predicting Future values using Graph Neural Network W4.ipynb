{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRDuJsGCgxCO"
      },
      "source": [
        "# Week4 Homework1 Graph Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Vxu1iQwdIE"
      },
      "source": [
        "# Check GPU Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKjf6sZcwb_A",
        "outputId": "2542165e-6715-482b-bcd6-e53c80e5d352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 22 06:31:03 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVgrPb3HhJUT"
      },
      "source": [
        "# Download Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAO6dg9eVaU_",
        "outputId": "81a7eb7c-066f-4b34-e215-90318618e855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.1.0\n",
            "    Uninstalling gdown-5.1.0:\n",
            "      Successfully uninstalled gdown-5.1.0\n",
            "Successfully installed gdown-5.2.0\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1jp5XRat8h3w4fZ7nEmDXBCf7-dfUFl1S\n",
            "From (redirected): https://drive.google.com/uc?id=1jp5XRat8h3w4fZ7nEmDXBCf7-dfUFl1S&confirm=t&uuid=63662de0-814c-456f-a474-1723f937cadb\n",
            "To: /content/data.zip\n",
            "100% 61.9M/61.9M [00:01<00:00, 46.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown --upgrade\n",
        "!gdown --id '1jp5XRat8h3w4fZ7nEmDXBCf7-dfUFl1S' --output data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEsBm1lkhGmk",
        "outputId": "f9f7820e-d2bd-4920-a6a2-2348c1bcfc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/features_test.pkl  \n",
            "  inflating: dataset/features_train.pkl  \n",
            "  inflating: dataset/graph_test.pkl  \n",
            "  inflating: dataset/graph_train.pkl  \n"
          ]
        }
      ],
      "source": [
        "! unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5ceUnRihL-f"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdpsFeT1P6wW",
        "outputId": "bf55b9cd-dced-4131-d70a-f47309f0ace8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwOGtRWHVaVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8cc3ad-634a-4b3e-8da9-945de24ee3fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x792541b2eaa0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import easydict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import json\n",
        "import math\n",
        "import copy, os\n",
        "\n",
        "import scipy.sparse as sp\n",
        "from scipy.stats import rankdata\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import torch\n",
        "import torch_geometric as tg\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.modules.loss import MSELoss\n",
        "from torch_geometric.nn.conv import GATConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usable Functions\n",
        "\n",
        "We use these functions to calculate the loss so we can better tune our model."
      ],
      "metadata": {
        "id": "HbJunzWRPzPH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kJm9GekVaVH"
      },
      "outputs": [],
      "source": [
        "def check_directories(directories):\n",
        "    for directory in directories:\n",
        "        if not os.path.exists(\"./\" + directory):\n",
        "            os.makedirs(\"./\" + directory)\n",
        "\n",
        "def to_device(batch, device):\n",
        "    data = copy.deepcopy(batch)\n",
        "    idx, graphs, labels = data.values()\n",
        "    data[\"idx\"] = idx[0].to(device)\n",
        "    data[\"hist_graphs\"] = [x.to(device) for x in graphs]\n",
        "    data[\"labels\"] = labels[0].to(device)\n",
        "    return data\n",
        "\n",
        "def mape_clip(label, pred, threshold = 0.1):\n",
        "    v = np.clip(np.abs(label), threshold, None)\n",
        "    diff = np.abs((label - pred) / v)\n",
        "    return 100.0 * np.mean(diff)\n",
        "\n",
        "def rankic(y_true, y_pred):\n",
        "    rank_true = len(y_true) - rankdata(y_true)+1\n",
        "    rank_pred = len(y_pred) - rankdata(y_pred)+1\n",
        "    res = np.corrcoef(rank_true, rank_pred)[0,1]\n",
        "    return res\n",
        "\n",
        "def calculate_perf(pred, label):\n",
        "    mae = mean_absolute_error(label, pred)\n",
        "    mape = mape_clip(label, pred)\n",
        "    ic = np.corrcoef(label, pred)[0,1]\n",
        "    rank_ic = rankic(label, pred)\n",
        "    return [mae, mape, ic, rank_ic]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Datasets\n",
        "\n",
        "The datasets are saved in pickle format. After loading it, we still need to preprocess the data.\n",
        "\n",
        "The training dataset contains 94 features, whereas the testing dataset has only 93 features. This discrepancy is due to the first column of the training dataset, which is the label, being removed from the testing dataset."
      ],
      "metadata": {
        "id": "QG-1exCtQTfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AssetBatch(Dataset):\n",
        "    def  __init__(self, args, adjs, features, labels,\n",
        "                  start, end):\n",
        "        super(AssetBatch, self).__init__()\n",
        "        self.args = args\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.adjs = adjs\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.hist_time_steps = args.hist_time_steps\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.end-self.start + 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = index+self.start\n",
        "        hist_graphs = []\n",
        "        for i in range(idx - self.args.hist_time_steps, idx):\n",
        "            x = torch.Tensor(self.features[i])\n",
        "            edge_index, edge_weight = tg.utils.from_scipy_sparse_matrix(self.adjs[i])\n",
        "\n",
        "            graph = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
        "            hist_graphs.append(graph)\n",
        "        sample = {'idx': torch.Tensor([idx]),\n",
        "                'hist_graphs': hist_graphs,\n",
        "                'labels': torch.Tensor(self.labels[idx])}\n",
        "        return sample\n",
        "\n",
        "    def subset(self, start_loc, end_loc):\n",
        "        adjusted_start = self.start + start_loc\n",
        "        adjusted_end = self.start + end_loc\n",
        "        return AssetBatch(self.args, self.adjs, self.features, self.labels, adjusted_start, adjusted_end)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AssetDataset():\n",
        "    def __init__(self, args, features, adjs, mode=\"train\"):\n",
        "        super(AssetDataset, self).__init__()\n",
        "        self.args = args\n",
        "        self.max_steps = len(adjs)\n",
        "        self.hist_time_steps = args.hist_time_steps\n",
        "        if mode == \"train\":\n",
        "            self.labels = [self._preprocess_labels(feat) for feat in features]\n",
        "            self.features = [self._preprocess_features(feat) for feat in features]\n",
        "            self.adjs = [self._preprocess_adj(a) for a in adjs]\n",
        "            self._split_data()\n",
        "        elif mode == \"test\":\n",
        "            self.labels = len(features) * [0]\n",
        "            self.features = [self._preprocess_features(feat) for feat in features]\n",
        "            self.adjs = [self._preprocess_adj(a) for a in adjs]\n",
        "            self._test_data()\n",
        "\n",
        "    def _preprocess_labels(self, features):\n",
        "        \"\"\"split return (label) from features\"\"\"\n",
        "        features = np.array(features.todense())\n",
        "        labels = features[:,0]\n",
        "        return labels\n",
        "\n",
        "    def _preprocess_features(self, features):\n",
        "        features = np.array(features.todense())\n",
        "        if self.args.feat_norm:\n",
        "            rowsum = np.array(features.sum(1))\n",
        "            r_inv = np.zeros_like(rowsum).flatten()\n",
        "            non_zero_indices = rowsum != 0\n",
        "            r_inv[non_zero_indices] = np.power(rowsum[non_zero_indices], -1).flatten()\n",
        "            r_mat_inv = sp.diags(r_inv)\n",
        "            features = r_mat_inv.dot(features)\n",
        "            return features\n",
        "        return features\n",
        "\n",
        "    def _preprocess_adj(self, adj):\n",
        "        if self.args.adj_norm:\n",
        "            \"\"\"normalization of adjacency matrix (scipy sparse format). Output is in tuple format\"\"\"\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            r_inv = sp.diags(np.power(rowsum, -1).flatten(), dtype=np.float32)\n",
        "            adj_normalized = r_inv.dot(adj)\n",
        "            return adj_normalized\n",
        "        return adj\n",
        "\n",
        "    def _split_data(self):\n",
        "        train_start = self.args.hist_time_steps\n",
        "        valid_start = int(np.floor(self.max_steps * self.args.train_proportion))\n",
        "\n",
        "\n",
        "        train = AssetBatch(self.args, self.adjs, self.features, self.labels,\n",
        "                           train_start, valid_start-1)\n",
        "        valid = AssetBatch(self.args, self.adjs, self.features, self.labels,\n",
        "                           valid_start + self.args.hist_time_steps, self.max_steps-1)\n",
        "\n",
        "        self.train = DataLoader(train, shuffle=True, batch_size = self.args.batch_size)\n",
        "        self.valid = DataLoader(valid, shuffle=False)\n",
        "\n",
        "\n",
        "        print('Dataset splits: ')\n",
        "        print('{:<3} train samples from {:<3} to {:<3}'.format(len(self.train), train_start, valid_start-1))\n",
        "        print('{:<3} valid samples from {:<3} to {:<3}'.format(len(self.valid), valid_start + self.args.hist_time_steps, self.max_steps-1))\n",
        "\n",
        "\n",
        "    def _test_data(self):\n",
        "        test_start = self.args.hist_time_steps\n",
        "        test = AssetBatch(self.args, self.adjs, self.features, self.labels,\n",
        "                           test_start, self.max_steps-1)\n",
        "        self.test = DataLoader(test, shuffle=False)\n",
        "\n",
        "        print('Dataset info: ')\n",
        "        print('{:<3} test samples from {:<3} to {:<3}'.format(len(self.test), test_start, self.max_steps-1))"
      ],
      "metadata": {
        "id": "O1gv24VpQXDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9MVtgbSVaVH"
      },
      "source": [
        "# Graph Attention Networks\n",
        "\n",
        "Feel free to edit your version of the model. You can check more details about GAT [here](https://arxiv.org/pdf/1710.10903)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvI3Xmq4VaVJ"
      },
      "outputs": [],
      "source": [
        "class GATconv(torch.nn.Module):\n",
        "    def __init__(self, args, node_features):\n",
        "        super(GATconv, self).__init__()\n",
        "        self.args = args\n",
        "        self.num_time_steps = args.hist_time_steps\n",
        "        self.num_features = node_features\n",
        "        self.static = GATConv(node_features, 16, 4)\n",
        "        self.linear = torch.nn.Linear(64, 1)\n",
        "        self.mseloss = MSELoss()\n",
        "\n",
        "    def forward(self, graphs):\n",
        "        x = []\n",
        "        for t in range(0, self.num_time_steps):\n",
        "            snapshot = graphs[t]\n",
        "            x.append(snapshot.x[:,0])\n",
        "        x = torch.stack(x, dim=1)\n",
        "        edge_index = snapshot.edge_index\n",
        "        edge_weight = snapshot.edge_weight\n",
        "        h = self.static(x, edge_index)\n",
        "        y = F.relu(h)\n",
        "        y = self.linear(y).flatten()\n",
        "        return y\n",
        "\n",
        "    def get_loss(self, data, chosen=None): # data: (N)\n",
        "        idx, graphs, labels = data.values()\n",
        "        # run gnn\n",
        "        pred = self.forward(graphs)\n",
        "\n",
        "        next_pred = pred\n",
        "        next_labels = labels\n",
        "\n",
        "        graphloss = self.mseloss(next_pred,next_labels)\n",
        "        return graphloss, next_pred.detach().cpu().numpy(), next_labels.detach().cpu().numpy()\n",
        "\n",
        "    def predict(self, data):\n",
        "        idx, graphs, labels = data.values()\n",
        "\n",
        "        predictions = self.forward(graphs)\n",
        "        return predictions.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Args you can use"
      ],
      "metadata": {
        "id": "mxZilBt6Qty_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = easydict.EasyDict({\n",
        "    \"hist_time_steps\": 12,\n",
        "    \"dataset\": \"dataset\",\n",
        "    \"GPU_ID\": 0,\n",
        "    \"model\": \"GAT\",\n",
        "    \"epochs\": 3,\n",
        "    \"val_freq\": 1,\n",
        "    \"test_freq\": 1,\n",
        "    \"batch_size\": 1,\n",
        "    \"feat_norm\": True,\n",
        "    \"adj_norm\": True,\n",
        "    \"early_stop\": 30,\n",
        "    \"train_proportion\": 0.7,\n",
        "    \"valid_proportion\": 0.15,\n",
        "    \"residual\": True,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"spatial_drop\": 0.1,\n",
        "    \"temporal_drop\": 0.5,\n",
        "    \"weight_decay\": 0.0005,\n",
        "    \"node_dim\": 8,\n",
        "    \"n_heads\": 16,\n",
        "    \"attention_layers\": 1,\n",
        "    \"centrality\": True,\n",
        "    \"spatial\": True,\n",
        "    \"edge\": True\n",
        "})"
      ],
      "metadata": {
        "id": "ocY1D_ZNQuVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Directories\n",
        "\n",
        "This is where you store your results."
      ],
      "metadata": {
        "id": "A2Eu_4ahQ1W_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBdtPhKwVaVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d32fed9-8229-4a50-cde2-3faab0c22d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results directory: result/\n"
          ]
        }
      ],
      "source": [
        "RESULTS_DIR = \"result/\"\n",
        "check_directories([RESULTS_DIR])\n",
        "with open(RESULTS_DIR+'/args.txt', 'w') as f:\n",
        "    json.dump(args.__dict__, f, indent=2)\n",
        "print('Results directory:', RESULTS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPFkDwug61PZ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_kDECOJVaVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80eae566-3c0c-443c-b87a-ce47e8df5ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Traning...\n",
            "Total time steps: 175\n",
            "Total number of assets: 372\n",
            "Total number of features: 94\n",
            "Dataset splits: \n",
            "110 train samples from 12  to 121\n",
            "41  valid samples from 134 to 174\n",
            "Epoch 0  , lr = 0.0001,  Train Loss = 0.0080, Valid Loss = 0.0057, Val MAE = 0.0594, Val MAPE = 51.7502, Val RMSE = 0.0743. \n",
            "Epoch 1  , lr = 0.0001,  Train Loss = 0.0079, Valid Loss = 0.0057, Val MAE = 0.0593, Val MAPE = 51.6205, Val RMSE = 0.0741. \n",
            "Epoch 2  , lr = 0.0001,  Train Loss = 0.0079, Valid Loss = 0.0057, Val MAE = 0.0590, Val MAPE = 51.4464, Val RMSE = 0.0739. \n"
          ]
        }
      ],
      "source": [
        "print(\"Start Traning...\")\n",
        "\n",
        "with open('dataset/graph_train.pkl', 'rb') as file:\n",
        "    adjs = pkl.load(file)\n",
        "with open('dataset/features_train.pkl', 'rb') as file:\n",
        "    feats = pkl.load(file)\n",
        "\n",
        "feat_dim = feats[0].shape[1]\n",
        "num_nodes = adjs[0].shape[0]\n",
        "\n",
        "print('Total time steps:', len(adjs))\n",
        "print('Total number of assets:', num_nodes)\n",
        "print('Total number of features:', feat_dim)\n",
        "\n",
        "# total time steps used for train, eval and test\n",
        "assert args.hist_time_steps < len(adjs), \"Time steps is illegal\"\n",
        "\n",
        "# build dataloader and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "dataloader = AssetDataset(args, feats, adjs, \"train\")\n",
        "\n",
        "\n",
        "model = GATconv(args, args.hist_time_steps).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "\n",
        "# training\n",
        "best_epoch_val, best_epoch = 1000000, 0\n",
        "patient = 0\n",
        "for epoch in range(args.epochs):\n",
        "    model.train()\n",
        "    epoch_loss = []\n",
        "    for idx, train_data in enumerate(dataloader.train):\n",
        "        train = to_device(train_data, device)\n",
        "        opt.zero_grad()\n",
        "        loss, _, _ = model.get_loss(train)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "    # get evaluation results\n",
        "    model.eval()\n",
        "    eval_loss, eval_rmse, eval_mae, eval_mape = [], [], [], []\n",
        "    for idx, valid_data in enumerate(dataloader.valid):\n",
        "        valid = to_device(valid_data, device)\n",
        "        eval_loss_idx, eval_pred, eval_labels = model.get_loss(valid)\n",
        "        eval_mae_idx, eval_mape_idx, _, _ = calculate_perf(eval_pred, eval_labels)\n",
        "        eval_loss.append(eval_loss_idx.detach().cpu().numpy())\n",
        "        eval_rmse.append(math.sqrt(eval_loss_idx.detach().cpu().numpy()))\n",
        "        eval_mae.append(eval_mae_idx)\n",
        "        eval_mape.append(eval_mape_idx)\n",
        "\n",
        "    if np.mean(eval_loss) < best_epoch_val:\n",
        "        best_epoch_val = np.mean(eval_loss)\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), RESULTS_DIR+\"/model.pt\")\n",
        "        patient = 0\n",
        "    else:\n",
        "        patient += 1\n",
        "        if patient > args.early_stop:\n",
        "            break\n",
        "\n",
        "    print(\"Epoch {:<3}, lr = {:<5},  Train Loss = {:.4f}, Valid Loss = {:.4f}, Val MAE = {:.4f}, Val MAPE = {:.4f}, Val RMSE = {:.4f}. \".format(\\\n",
        "            epoch, opt.param_groups[0][\"lr\"], np.mean(epoch_loss), np.mean(eval_loss),\\\n",
        "            np.mean(eval_mae), np.mean(eval_mape), np.mean(eval_rmse)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgnIOaID687b"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_OeWtstVaVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "678802f0-7b0e-44a6-e15d-393b46c5ede2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Testing...\n",
            "Dataset info: \n",
            "21  test samples from 12  to 32 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-ced60d7e01ac>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(RESULTS_DIR+\"/model.pt\"))\n",
            "<ipython-input-12-ced60d7e01ac>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(RESULTS_DIR+\"/model.pt\"))\n"
          ]
        }
      ],
      "source": [
        "print(\"Start Testing...\")\n",
        "# Prediction by Best Model\n",
        "model.load_state_dict(torch.load(RESULTS_DIR+\"/model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "with open('dataset/graph_test.pkl', 'rb') as file:\n",
        "    adjs_test = pkl.load(file)\n",
        "with open('dataset/features_test.pkl', 'rb') as file:\n",
        "    feats_test = pkl.load(file)\n",
        "feat_dim = feats_test[0].shape[1]\n",
        "num_nodes = adjs_test[0].shape[0]\n",
        "\n",
        "dataloader = AssetDataset(args, feats_test, adjs_test, \"test\")\n",
        "\n",
        "\n",
        "# Test Best Model\n",
        "model.load_state_dict(torch.load(RESULTS_DIR+\"/model.pt\"))\n",
        "model.eval()\n",
        "test_preds = []\n",
        "for idx, test_data in enumerate(dataloader.test):\n",
        "    test = to_device(test_data, device)\n",
        "    test_pred_idx = model.predict(test)\n",
        "    test_preds.append(test_pred_idx)\n",
        "\n",
        "# write to csv\n",
        "pred = pd.DataFrame(test_preds)\n",
        "stacked_pred = pred.stack().reset_index(drop=True)\n",
        "stacked_pred = stacked_pred.reset_index()\n",
        "stacked_pred.columns = ['Id', 'Label']\n",
        "stacked_pred.to_csv(RESULTS_DIR+'/test_pred.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}